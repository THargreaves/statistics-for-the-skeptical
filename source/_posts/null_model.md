---
title: 'The Null Model'
date: '2019-11-24'
---

## Hello World

Hello and welcome to the first of hopefully many posts for my new blog site, 'Statistics for the Skeptical'. 

The goal of this project is to share some key statistical concepts with you, the reader; not for the purpose of sheer curiosity (though if that is motivation enough for you—welcome to the club) but to reduce your chance of being fooled by deceitful data, sly statistics, and guileful graphs. After all, we are transitioning into an economy driven by data, and statistics is the natural language of such a world. If we do not invest some time into learning this language, we leave ourselves vulnerable to having our objectives 'lost in translation', or worst, getting taken advantage of by those with the gift of the statistical gab.

Each post in this series will discuss a real-life example in which the poor application of statistics could have led to miscommunication, false-comfort, or trickery. These aren't your classic textbook examples thought up by some out-of-touch professor; these are real problems that I have witnessed in my work, the media, and my conversations with friends. We will introduce these examples and walk through how a small amount of statistical know-how can provide add clarity to each situation. Most importantly, I then offer some tips on how you can avoid being tricked when you come across similar scenarios in your life.

That's enough inauguration. Let's get the ball rolling with the first example of how poor statistics practice can lead to trouble. In this case, we will be looking at a time when the failure to check a model's accuracy almost led to throwing away £5,000 on a piece of predictive software that was worse than blind guessing .

## A Costly Oversight

Before we get started, I want to reiterate that this is not some made-up anecdote crafted to illustrate my point. In fact, it is the exact opposite; this is true tale that shocked me so much, that I couldn't help but write this post to share it in the hope that doing so will make it is less likely to happen again. Due to the sensitive nature of this topic, the names of the companies involved in this story and the exact details of the statistical model we are discussing will have to be omitted. This is not of critical importance though, as this sort of mistake could have occurred in any industry, in any company, and in fact, I'm sure it is on a regular basis. To avoid any confusion that may arise from talking about multiple name-less entities, I will make up an imaginary scenario which, though a figment of my imagination, completely captures the true desires and behaviours of the actors involved in the real-world equivalent.

The company SleepEasy is the imaginary protagonist (though of the foolish sort) in our story. One of the projects they are working on involves taking images of sheep-filled meadows and counting how many of our ovine-acquaintances are in each picture. Previously, this was a manual process in which highly skilled workers were having to sacrifice their limited (and expensive) time to sit and count sheep by hand. The benefit of such a method is that we can be rather certain that these workers will come back with the correct number of sheep. On the other hand, the sheer man-power required to perform such a task on a regular basis easily outweighs the positives to make the whole process rather undesirable from a business perspective. It is therefore unsurprising that SleepEasy were keen to automate this process and so, when they received a proposal from an external company called DeepData offering to build a machine learning solution to solve this problem, they virtually jumped at the offer.

### Warning Signs

In the exchanges that followed, many extravagant promises were made with all the usual data science buzzwords (AI, deep learning, neural network, and the like) thrown in for good measure. Before long, a deal was reached with a hefty price tag of £5,000 to accompany it. In exchange, DeepData would develop a custom software solution which would take the woolly images described above and automatically generate predictions of how many sheep were in each picture. When pushed by SleepEasy regarding how exactly this piece of software will perform this task, DeepData promptly returned graphics almost identical to the following.

![An non-descript flowchart of DeepData's modelling process](/images/neuralnetworkflowchart.jpg)

Frankly, the middle textbox might as well say 'Magic/Pixie Dust', offering no insight at all into how the model is built.

Unfortunately, I was yet to become a part of this story; I only discovered these graphics when I traced my way back down the email chain in hindsight. If I'd have seen this when it was sent, it would have immediately set off alarm bells in my head. In my experience with building complicated statistical and machine learning models, it is almost always possible to find an explanation of how a model works that is accessible to non-technical users without giving away any inner workings. There is no reason why this time should have been any different, and so for me, this resistance to give a clear explanation would have made me highly suspicious of the model's validity.

Despite this, SleepEasy decided to push ahead with the project. No money changed hands just yet, but the deal was moving into its final evaluation stage. This involved SleepEasy sending over twenty images in which the number of sheep had been manually determined by some poor (and probably desperate-for-a-nap) employee. DeepData would then run their software on the same images and generate their predictions for the sheep count. It is worth noting that these are only predictions. No one was expecting these counts to be completely accurate. After all, computer vision is a notoriously difficult task. The value added in the speed of this process. Whereas it takes a human labeller a minute or two per image to reach their counts, a computer can do this in a matter of seconds. And if the computer is _slightly_ off, well it isn't the end of the world as the long as these errors generally float around the true number of sheep. This is where I join the story, CC'ed into an email containing the predictions from this model. As a passionate statistician, I was curious to interrogate these results. I opened the attached PDF, and went in, completely unprepared for what I was about to find.

### Poking Holes

So there I am, taking a break from my usual work to look at a report for a project I'm only tangentially involved in, presented with a table containing the twenty manually labelled sheep counts each paired up with a corresponding prediction from the DeepData model. All of the predictions seem reasonably close to the human-obtained counts, but I know from experience that looks can be deceiving. The DeepData team have also included some calculations to showcase their performance. This consisted of, for each image they were tested on, calculating the percentage accuracy of their model against the true counts. For example, if their model had predicted that there were $94$ sheep in the picture but in fact there were $100$, they would have assigned themselves an accuracy of $94\\%$ for that image. In the case where they predicted more sheep than there were, this was reversed. For example, if the model predicted there were $100$ sheep but there were only $88$, this would correspond to an accuracy of $88\\%$. It is worth mentioning that this is a *very* strange metric to use in this scenario<sup><a id='link1' href='#footnote1'>[1]</a></sup>. This would have been cause for concern in itself, but it turns out that there was a much more severe problem lurking just below the surface.

With these twenty accuracy measures, DeepData then took the average to come up with an overall measure of their model's performance. This came out to be $93\\%$. Sounds good, right? For a moment I thought so too, but I decided to verify this just to be sure. My motivation for this check was that model accuracy often doesn't tell you the full story. For a difficult prediction task, $93\\%$ would be an impressive score; yet if the problem was something so simple it was almost trivial, we might be hoping for something a bit higher. The only way we can know for sure is by comparing the performance of the model to what is known as the **null model**.

The null model can roughly be described as the best performing<sup><a id='link2' href='#footnote2'>[2]</a></sup> model, that doesn't use any of the predictors available. In this case the lone predictor we have to use is the image in question and so the null model would be the best possible predictions we could make without even looking at the pictures we are given. It is obvious how we could make a bad model without looking at the images—simply guess that there are three trillion-or-so sheep in each picture; we will almost certainly be off by a few orders of magnitude each time. The question of how we come up with the best of such models is a bit more subtle. The way we typically do this is by taking the average of the human counts and using this as our prediction for every single image. That way we create a model that predicts exactly the same sheep count no matter what input it is given. The nuance is that we choose this constant value in such a way that it has the best chance of coming close to the true value. That is, the null model represents the best possible random guess. Remember that in building and training their model, DeepData would also have used these human labels to make their model 'learn' and so it isn't like we're doing anything special by using this information to create the null model.

This is exactly what I did. I computed the mean of the given counts, used this as my prediction for every single image, calculated accuracy in the same way as DeepData did before, and took the average over all twenty pictures. The result—$96\\%$. "Wow", I thought. That's $3\\%$ higher than the model that DeepData were trying to sell us. To elaborate on this, we would typically expect a model to perform significantly better than the null model. If a model performed about as well as the null model, then that is to say that our model is no better than guessing. Admittedly, we are talking about the most educated guessing, but this still means that a model performing as well the null model is no better than random chance. To have a model perform _worse_ than the null model is shocking; a model that is _worse_ than literally guessing the outcome<sup><a id='link3' href='#footnote3'>[3]</a></sup>.

I went and checked in with someone from the team responsible for organising this deal. Maybe I was misinterpreting the data. Perhaps there was some subtlety I wasn't taking into account. I wish. It turns out that my analysis was spot on and that no one had thought to check. After all, $93\\%$ accuracy looks great, but looks can be deceiving. After explaining the concept of the null model to this team, the deal was put to rest. It is scary to think that if I had been too busy to peruse this report, that might have been £5,000 thrown away on something with a predictive power worse than a magic 8-ball.

## Cloudy with a Chance of Statistics

The timeline shown above is perhaps a bit misleading, making it look a bit too much like my concerns were accepted quickly. In fact, it took quite a bit of convincing and explanation through analogy before I could persuade anyone that my view did hold up. And to that extent, I am making no assumption that I have fully convinced you that there was anything wrong with the DeepData model discussed above. I will therefore provide another shorter example; the same as one that I told the imaging team at 'SleepEasy'. This takes the problem highlighted above to an absolute extreme to demonstrate what is wrong with a model that performs worse than the null model. Even though this is an exaggerated case, I hope you will be able to see that the key point still applies: that is, a high accuracy alone means nothing, it all depends on how large it is relative to the best guessing you can do.

Suppose for a moment that you have packed up youl belongings and emigrated to the Atacama Desert to begin a new life. Putting the madness of this decision aside, your biggest worry is likely concerned with the amount of rainfall the region typically receives. As luck might have it, a travelling salesman is passing your way, offering their wares of gadgets and gismos for predicting whether rainfall is due. The salesman begins to showcase his best-selling product, a nifty tool which he proclaims can predict whether it will rain on the next day with an accuracy of over $95\%$. "And it can be yours", he says, "for the low, low price of five million Chilean Pesos—around £5,000"<sup><a id='link4' href='#footnote4'>[4]</a></sup>. For a moment that may sound like a reasonable offer; knowledge of tomorrow's rainfall could be the difference between life and death, and £5,000 seems like a small price to pay for that.

Before you hand over the cash, you pause for a moment and think. The Atacama Desert is one of the driest places on the globe. Some weather stations are yet to have received any rain and some evidence suggests that there may be regions that have not received meaningful rainfall for spans as long as four centuries<sup><a id='link5' href='#footnote5'>[5]</a></sup>. Because of this, if you were to create a statistical model which simply predicted that it never rained, this would have a corresponding accuracy of well over $99.9\\%$. Knowing that, $95\\%$ doesn't sound so good anymore and so you put away your cash and the travelling salesman continues on his way.

We will leave this story here, but the curious may be interested in learning about some alternative metrics to accuracy which help overcome this issue. A link to such further reading can be found in the footnotes<sup><a id='link6' href='#footnote6'>[6]</a></sup>. The important take-away with this story is that as soon as you can obtain a better performance than a statistical model just by always giving the same, well-chosen prediction, the model in question is of absolutely no value at all.

## Action Plan

Hopefully by this point you are convinced—a model that performs worse than the null model has no value. And even then, a minor improvement on the null model is not worth much either. Despite this, there will always be companies, politicians, and marketers flouting models with high accuracy, that are in reality worse than or barely on improvement on the null model. So how can you avoid being fooled by this? Here a few key ideas to keep in mind:

1. A high accuracy means nothing. You should only consider it when evaluating a statistical model's performance if you have a benchmark to compare it to, such as a null model, or an existing model that you are looking to replace.

2. When possible, calculate the performance metrics you care about for the null model too. This may be as simple as putting your data into Excel and finding the average of your training data before calculating the metrics of interest for these new values.

3. When in doubt, ask a statistician or data scientist to generate corresponding metrics for the null model. If you show interest in model integrity, they will most likely be willing to perform further checks for you too.

4. Never be afraid to ask a salesman to benchmark their solution against the null model. It takes minutes for this to be performed if they know what they're doing. If they refuse, then either there is something fishy going on, or they may just not be aware of the concept—in which case, you might want to send them this article!

With that advice, it is time to wrap up this post. I look forward to joining you again for the next post. This will look at how a news media with an appetite for the sensational can mislead people into misjudging the severity of particular social issues. 

Until then, **stay skeptical**.

<hr>

#### Footnotes

<a id='footnote1' href='#link1'>[1]</a> Typically accuracy is a metric used for evaluating the prediction of a categorical variable—that is, an outcome that can take one of a finite number of possible options. In this case, accuracy is being used to evaluate the prediction of a discrete variable which can (theoretically) take on an infinite number of values. The main issue with this approach is that we lose the symmetry that we typically expect from a performance metric. A prediction that is off by $50$ sheep should be just as bad if we over or under-estimate. That is not the case when we use accuracy as our metric. Suppose the true number of sheep is $100$. Guessing $50$ too low will give us a $50\\%$ accuracy yet guessing $50$ too high gives us roughly $67\\%$. A much more appropriate metric would have been root mean squared error (or RMSE). This metric is the first port of call for this type of data and is generated by calculating how far each prediction is from the true value, squaring these differences, taking the average over all images, and square rooting the result. This has the desired symmetry property, as well as having other nice behaviour such as punishing a few large errors more than several small errors.

<a id='footnote2' href='#link2'>[2]</a> This is an extremely loose definition of the null model but suffices for our purposes. In fact, the null model is better defined as the best model under the hypothesis that the outcome of the model is not at all influenced by the inputs. When we talk about a 'best' model we are generally referring to a model that minimises the RMSE mentioned in the first footnote.

<a id='footnote3' href='#link3'>[3]</a> The inquisitive reader may be wondering if this problem only arose when using the ill-advised accuracy metric. This is something I questioned too and so I also checked that the issue remained when I used more typical metrics (such as RMSE mentioned in the first footnote). In all cases I tried the problem remained, showing that fault was more with the model than the questionable choice in metrics.

<a id='footnote4' href='#link4'>[4]</a> At the time of writing this post, 1 Pound sterling equals 1,002.20 Chilean Peso.

<a id='footnote5' href='#link5'>[5]</a> Source: https://archive.org/details/newyorktimes200600wrig

<a id='footnote6' href='#link6'>[6]</a> Unless you are working with perfectly balanced classes, accuracy can be a misleading metric for success. There are many alternatives which give a much clearer definition of a 'good' model such as balanced accuracy and the $F_1$-score. For a more detailed discussion of this topic, please refer to [this article](https://wwww.ttested.com/inaccuacy-of-accuracy) on my data science blog, [T-Tested](https://www.ttested.com).
